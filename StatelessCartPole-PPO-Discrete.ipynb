{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de31423b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "from gym.utils import seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7b32a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatelessCartPole(gym.Env):\n",
    "    \"\"\"Partially observable variant of the CartPole gym environment.\n",
    "\n",
    "    https://github.com/openai/gym/blob/master/gym/envs/classic_control/\n",
    "    cartpole.py\n",
    "\n",
    "    We delete the velocity component of the state, so that it can only be\n",
    "    solved by a LSTM policy.\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\n",
    "        \"render.modes\": [\"human\", \"rgb_array\"],\n",
    "        \"video.frames_per_second\": 60\n",
    "    }\n",
    "\n",
    "    def __init__(self, config=None):\n",
    "        self.gravity = 9.8\n",
    "        self.masscart = 1.0\n",
    "        self.masspole = 0.1\n",
    "        self.total_mass = (self.masspole + self.masscart)\n",
    "        self.length = 0.5  # actually half the pole's length\n",
    "        self.polemass_length = (self.masspole * self.length)\n",
    "        self.force_mag = 10.0\n",
    "        self.tau = 0.02  # seconds between state updates\n",
    "\n",
    "        # Angle at which to fail the episode\n",
    "        self.theta_threshold_radians = 12 * 2 * math.pi / 360\n",
    "        self.x_threshold = 2.4\n",
    "\n",
    "        high = np.array([\n",
    "            self.x_threshold * 2,\n",
    "            self.theta_threshold_radians * 2,\n",
    "        ])\n",
    "\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        self.observation_space = spaces.Box(-high, high)\n",
    "\n",
    "        self.seed()\n",
    "        self.viewer = None\n",
    "        self.state = None\n",
    "\n",
    "        self.steps_beyond_done = None\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(\n",
    "            action), \"%r (%s) invalid\" % (action, type(action))\n",
    "        state = self.state\n",
    "        x, x_dot, theta, theta_dot = state\n",
    "        force = self.force_mag if action == 1 else -self.force_mag\n",
    "        costheta = math.cos(theta)\n",
    "        sintheta = math.sin(theta)\n",
    "        temp = (force + self.polemass_length * theta_dot * theta_dot * sintheta\n",
    "                ) / self.total_mass\n",
    "        thetaacc = (self.gravity * sintheta - costheta * temp) / (\n",
    "            self.length *\n",
    "            (4.0 / 3.0 - self.masspole * costheta * costheta / self.total_mass)\n",
    "        )\n",
    "        xacc = (temp -\n",
    "                self.polemass_length * thetaacc * costheta / self.total_mass)\n",
    "        x = x + self.tau * x_dot\n",
    "        x_dot = x_dot + self.tau * xacc\n",
    "        theta = theta + self.tau * theta_dot\n",
    "        theta_dot = theta_dot + self.tau * thetaacc\n",
    "        self.state = (x, x_dot, theta, theta_dot)\n",
    "        done = (x < -self.x_threshold or x > self.x_threshold\n",
    "                or theta < -self.theta_threshold_radians\n",
    "                or theta > self.theta_threshold_radians)\n",
    "        done = bool(done)\n",
    "\n",
    "        if not done:\n",
    "            reward = 1.0\n",
    "        elif self.steps_beyond_done is None:\n",
    "            # Pole just fell!\n",
    "            self.steps_beyond_done = 0\n",
    "            reward = 1.0\n",
    "        else:\n",
    "            self.steps_beyond_done += 1\n",
    "            reward = 0.0\n",
    "\n",
    "        rv = np.r_[self.state[0], self.state[2]]\n",
    "        return rv, reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.np_random.uniform(low=-0.05, high=0.05, size=(4, ))\n",
    "        self.steps_beyond_done = None\n",
    "\n",
    "        rv = np.r_[self.state[0], self.state[2]]\n",
    "        return rv\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        screen_width = 600\n",
    "        screen_height = 400\n",
    "\n",
    "        world_width = self.x_threshold * 2\n",
    "        scale = screen_width / world_width\n",
    "        carty = 100  # TOP OF CART\n",
    "        polewidth = 10.0\n",
    "        polelen = scale * 1.0\n",
    "        cartwidth = 50.0\n",
    "        cartheight = 30.0\n",
    "\n",
    "        if self.viewer is None:\n",
    "            from gym.envs.classic_control import rendering\n",
    "            self.viewer = rendering.Viewer(screen_width, screen_height)\n",
    "            l, r, t, b = (-cartwidth / 2, cartwidth / 2, cartheight / 2,\n",
    "                          -cartheight / 2)\n",
    "            axleoffset = cartheight / 4.0\n",
    "            cart = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])\n",
    "            self.carttrans = rendering.Transform()\n",
    "            cart.add_attr(self.carttrans)\n",
    "            self.viewer.add_geom(cart)\n",
    "            l, r, t, b = (-polewidth / 2, polewidth / 2,\n",
    "                          polelen - polewidth / 2, -polewidth / 2)\n",
    "            pole = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])\n",
    "            pole.set_color(.8, .6, .4)\n",
    "            self.poletrans = rendering.Transform(translation=(0, axleoffset))\n",
    "            pole.add_attr(self.poletrans)\n",
    "            pole.add_attr(self.carttrans)\n",
    "            self.viewer.add_geom(pole)\n",
    "            self.axle = rendering.make_circle(polewidth / 2)\n",
    "            self.axle.add_attr(self.poletrans)\n",
    "            self.axle.add_attr(self.carttrans)\n",
    "            self.axle.set_color(.5, .5, .8)\n",
    "            self.viewer.add_geom(self.axle)\n",
    "            self.track = rendering.Line((0, carty), (screen_width, carty))\n",
    "            self.track.set_color(0, 0, 0)\n",
    "            self.viewer.add_geom(self.track)\n",
    "\n",
    "        if self.state is None:\n",
    "            return None\n",
    "\n",
    "        x = self.state\n",
    "        cartx = x[0] * scale + screen_width / 2.0  # MIDDLE OF CART\n",
    "        self.carttrans.set_translation(cartx, carty)\n",
    "        self.poletrans.set_rotation(-x[2])\n",
    "\n",
    "        return self.viewer.render(return_rgb_array=mode == \"rgb_array\")\n",
    "\n",
    "    def close(self):\n",
    "        if self.viewer:\n",
    "            self.viewer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d4c5a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "from ray.rllib.utils.test_utils import check_learning_achieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71f0c308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--stop-reward'], dest='stop_reward', nargs=None, const=None, default=150.0, type=<class 'float'>, choices=None, help='Reward at which we stop training.', metavar=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    \"--run\",\n",
    "    type=str,\n",
    "    default=\"PPO\",\n",
    "    help=\"The RLlib-registered algorithm to use.\")\n",
    "parser.add_argument(\"--num-cpus\", type=int, default=0)\n",
    "parser.add_argument(\n",
    "    \"--framework\",\n",
    "    choices=[\"tf\", \"tf2\", \"tfe\", \"torch\"],\n",
    "    default=\"torch\",\n",
    "    help=\"The DL framework specifier.\")\n",
    "parser.add_argument(\"--eager-tracing\", action=\"store_true\")\n",
    "parser.add_argument(\"--use-prev-action\", action=\"store_true\")\n",
    "parser.add_argument(\"--use-prev-reward\", action=\"store_true\")\n",
    "parser.add_argument(\n",
    "    \"--as-test\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Whether this script should be run as a test: --stop-reward must \"\n",
    "    \"be achieved within --stop-timesteps AND --stop-iters.\")\n",
    "parser.add_argument(\n",
    "    \"--stop-iters\",\n",
    "    type=int,\n",
    "    default=200,\n",
    "    help=\"Number of iterations to train.\")\n",
    "parser.add_argument(\n",
    "    \"--stop-timesteps\",\n",
    "    type=int,\n",
    "    default=100000,\n",
    "    help=\"Number of timesteps to train.\")\n",
    "parser.add_argument(\n",
    "    \"--stop-reward\",\n",
    "    type=float,\n",
    "    default=150.0,\n",
    "    help=\"Reward at which we stop training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "793cb424",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-17 09:54:42,633\tWARNING services.py:1748 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67047424 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=10.24gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "\n",
    "args = parser.parse_args(\"--stop-iters=10 --use-prev-action --use-prev-reward\".split())\n",
    "\n",
    "ray.init(num_cpus=args.num_cpus or None)\n",
    "\n",
    "configs = {\n",
    "    \"PPO\": {\n",
    "        \"num_sgd_iter\": 5,\n",
    "        \"sgd_minibatch_size\": 128,\n",
    "        \"simple_optimizer\": True,        \n",
    "        \"model\": {\n",
    "            \"vf_share_layers\": True,\n",
    "        },\n",
    "        \"vf_loss_coeff\": 0.0001,\n",
    "    },\n",
    "    \"IMPALA\": {\n",
    "        \"num_workers\": 2,\n",
    "        \"num_gpus\": 0,\n",
    "        \"vf_loss_coeff\": 0.01,\n",
    "    },\n",
    "}\n",
    "\n",
    "config = dict(\n",
    "    configs[args.run],\n",
    "    **{\n",
    "        \"env\": StatelessCartPole,\n",
    "        # Use GPUs iff `RLLIB_NUM_GPUS` env var set to > 0.\n",
    "        \"num_gpus\": int(os.environ.get(\"RLLIB_NUM_GPUS\", \"0\")),\n",
    "        \"model\": {\n",
    "            \"use_lstm\": True,\n",
    "            \"lstm_cell_size\": 256,\n",
    "            \"lstm_use_prev_action\": args.use_prev_action,\n",
    "            \"lstm_use_prev_reward\": args.use_prev_reward,\n",
    "        },\n",
    "        \"framework\": args.framework,\n",
    "        # Run with tracing enabled for tfe/tf2?\n",
    "        \"eager_tracing\": args.eager_tracing,\n",
    "    })\n",
    "\n",
    "stop = {\n",
    "    \"training_iteration\": args.stop_iters,\n",
    "    \"timesteps_total\": args.stop_timesteps,\n",
    "    \"episode_reward_mean\": args.stop_reward,\n",
    "}\n",
    "\n",
    "# To run the Trainer without tune.run, using our LSTM model and\n",
    "# manual state-in handling, do the following:\n",
    "\n",
    "# Example (use `config` from the above code):\n",
    "# >> import numpy as np\n",
    "# >> from ray.rllib.agents.ppo import PPOTrainer\n",
    "# >>\n",
    "# >> trainer = PPOTrainer(config)\n",
    "# >> lstm_cell_size = config[\"model\"][\"lstm_cell_size\"]\n",
    "# >> env = StatelessCartPole()\n",
    "# >> obs = env.reset()\n",
    "# >>\n",
    "# >> # range(2) b/c h- and c-states of the LSTM.\n",
    "# >> init_state = state = [\n",
    "# ..     np.zeros([lstm_cell_size], np.float32) for _ in range(2)\n",
    "# .. ]\n",
    "# >> prev_a = 0\n",
    "# >> prev_r = 0.0\n",
    "# >>\n",
    "# >> while True:\n",
    "# >>     a, state_out, _ = trainer.compute_single_action(\n",
    "# ..         obs, state, prev_a, prev_r)\n",
    "# >>     obs, reward, done, _ = env.step(a)\n",
    "# >>     if done:\n",
    "# >>         obs = env.reset()\n",
    "# >>         state = init_state\n",
    "# >>         prev_a = 0\n",
    "# >>         prev_r = 0.0\n",
    "# >>     else:\n",
    "# >>         state = state_out\n",
    "# >>         prev_a = a\n",
    "# >>         prev_r = reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee618e48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_sgd_iter': 5,\n",
       " 'sgd_minibatch_size': 128,\n",
       " 'simple_optimizer': True,\n",
       " 'model': {'use_lstm': True,\n",
       "  'lstm_cell_size': 256,\n",
       "  'lstm_use_prev_action': True,\n",
       "  'lstm_use_prev_reward': True},\n",
       " 'vf_loss_coeff': 0.0001,\n",
       " 'env': __main__.StatelessCartPole,\n",
       " 'num_gpus': 0,\n",
       " 'framework': 'torch',\n",
       " 'eager_tracing': False}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b904902a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-17 09:54:48 (running for 00:00:00.16)<br>Memory usage on this node: 6.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/12 CPUs, 0/1 GPUs, 0.0/41.89 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/condauser/ray_results/PPO<br>Number of trials: 1/1 (1 PENDING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-17 09:54:48,504\tERROR syncer.py:75 -- Log sync requires rsync to be installed.\n",
      "\u001b[2m\u001b[36m(pid=6926)\u001b[0m 2021-11-17 09:54:50,189\tWARNING deprecation.py:38 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=6926)\u001b[0m 2021-11-17 09:54:50,189\tINFO trainer.py:770 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=6916)\u001b[0m /opt/conda/lib/python3.9/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=6916)\u001b[0m   logger.warn(\n",
      "\u001b[2m\u001b[36m(pid=6920)\u001b[0m /opt/conda/lib/python3.9/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=6920)\u001b[0m   logger.warn(\n",
      "\u001b[2m\u001b[36m(pid=6920)\u001b[0m 2021-11-17 09:54:51,855\tWARNING deprecation.py:38 -- DeprecationWarning: `SampleBatch['is_training']` has been deprecated. Use `SampleBatch.is_training` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=6926)\u001b[0m 2021-11-17 09:54:51,966\tWARNING deprecation.py:38 -- DeprecationWarning: `SampleBatch['is_training']` has been deprecated. Use `SampleBatch.is_training` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=6926)\u001b[0m 2021-11-17 09:54:51,988\tWARNING trainer_template.py:185 -- `execution_plan` functions should accept `trainer`, `workers`, and `config` as args!\n",
      "\u001b[2m\u001b[36m(pid=6926)\u001b[0m 2021-11-17 09:54:51,989\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-17 09:54:53 (running for 00:00:05.32)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/1 GPUs, 0.0/41.89 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/condauser/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=6926)\u001b[0m 2021-11-17 09:54:56,362\tWARNING deprecation.py:38 -- DeprecationWarning: `_get_slice_indices` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=6926)\u001b[0m 2021-11-17 09:54:56,365\tWARNING deprecation.py:38 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-17 09:54:58 (running for 00:00:10.35)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/1 GPUs, 0.0/41.89 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/condauser/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial PPO_StatelessCartPole_04a94_00000 reported episode_reward_max=74.0,episode_reward_min=9.0,episode_reward_mean=21.785714285714285,episode_len_mean=21.785714285714285,episode_media={},episodes_this_iter=182,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 0.14105000705326087, 'mean_inference_ms': 1.6528654878614037, 'mean_action_processing_ms': 0.055174711411258445, 'mean_env_wait_ms': 0.10674619796974744, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,timesteps_this_iter=0,agent_timesteps_total=4000,timers={'sample_time_ms': 4372.744, 'sample_throughput': 914.758, 'learn_time_ms': 4296.595, 'learn_throughput': 930.97, 'update_time_ms': 2.427},info={'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.19999999999999998, 'cur_lr': 5.000000000000001e-05, 'total_loss': -0.013530209939926863, 'policy_loss': -0.03782545167487115, 'vf_loss': 219.4263632297516, 'vf_explained_var': 0.0016991600394248962, 'kl': 0.011763041814778313, 'entropy': 0.6813680574297905, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}}}, 'num_steps_sampled': 4000, 'num_agent_steps_sampled': 4000, 'num_steps_trained': 4000},perf={'cpu_util_percent': 18.553846153846155, 'ram_util_percent': 11.800000000000002} with parameters={'num_sgd_iter': 5, 'sgd_minibatch_size': 128, 'simple_optimizer': True, 'model': {'use_lstm': True, 'lstm_cell_size': 256, 'lstm_use_prev_action': True, 'lstm_use_prev_reward': True}, 'vf_loss_coeff': 0.0001, 'env': <class '__main__.StatelessCartPole'>, 'num_gpus': 0, 'framework': 'torch', 'eager_tracing': False}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-17 09:55:03 (running for 00:00:15.52)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/1 GPUs, 0.0/41.89 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/condauser/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-17 09:55:08 (running for 00:00:20.54)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/1 GPUs, 0.0/41.89 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/condauser/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial PPO_StatelessCartPole_04a94_00000 reported episode_reward_max=85.0,episode_reward_min=11.0,episode_reward_mean=28.905797101449274,episode_len_mean=28.905797101449274,episode_media={},episodes_this_iter=138,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 0.13440105482905731, 'mean_inference_ms': 1.6066227094678602, 'mean_action_processing_ms': 0.05308624149352066, 'mean_env_wait_ms': 0.10312974283055269, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,timesteps_this_iter=0,agent_timesteps_total=8000,timers={'sample_time_ms': 6278.027, 'sample_throughput': 637.143, 'learn_time_ms': 4285.265, 'learn_throughput': 933.431, 'update_time_ms': 2.556},info={'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.19999999999999996, 'cur_lr': 5e-05, 'total_loss': 0.019632059053489657, 'policy_loss': -0.02342396839098497, 'vf_loss': 413.04342286081027, 'vf_explained_var': 0.00038034590807828035, 'kl': 0.008758444329466692, 'entropy': 0.6537184083100521, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}}}, 'num_steps_sampled': 8000, 'num_agent_steps_sampled': 8000, 'num_steps_trained': 8000, 'num_steps_trained_this_iter': 0},perf={'cpu_util_percent': 18.55, 'ram_util_percent': 11.850000000000001} with parameters={'num_sgd_iter': 5, 'sgd_minibatch_size': 128, 'simple_optimizer': True, 'model': {'use_lstm': True, 'lstm_cell_size': 256, 'lstm_use_prev_action': True, 'lstm_use_prev_reward': True}, 'vf_loss_coeff': 0.0001, 'env': <class '__main__.StatelessCartPole'>, 'num_gpus': 0, 'framework': 'torch', 'eager_tracing': False}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-17 09:55:13 (running for 00:00:25.69)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/1 GPUs, 0.0/41.89 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/condauser/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial PPO_StatelessCartPole_04a94_00000 reported episode_reward_max=119.0,episode_reward_min=11.0,episode_reward_mean=42.75,episode_len_mean=42.75,episode_media={},episodes_this_iter=89,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 0.1334640650507028, 'mean_inference_ms': 1.6383927345005065, 'mean_action_processing_ms': 0.05403298747732385, 'mean_env_wait_ms': 0.10464187628147618, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,timesteps_this_iter=0,agent_timesteps_total=12000,timers={'sample_time_ms': 7064.135, 'sample_throughput': 566.241, 'learn_time_ms': 4323.192, 'learn_throughput': 925.242, 'update_time_ms': 2.413},info={'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.19999999999999996, 'cur_lr': 5e-05, 'total_loss': 0.03469155574443214, 'policy_loss': -0.03375715291494447, 'vf_loss': 668.4830734715317, 'vf_explained_var': -0.0036347978042833733, 'kl': 0.008002017603022545, 'entropy': 0.6214002121578563, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}}}, 'num_steps_sampled': 12000, 'num_agent_steps_sampled': 12000, 'num_steps_trained': 12000, 'num_steps_trained_this_iter': 0},perf={'cpu_util_percent': 18.925, 'ram_util_percent': 11.9} with parameters={'num_sgd_iter': 5, 'sgd_minibatch_size': 128, 'simple_optimizer': True, 'model': {'use_lstm': True, 'lstm_cell_size': 256, 'lstm_use_prev_action': True, 'lstm_use_prev_reward': True}, 'vf_loss_coeff': 0.0001, 'env': <class '__main__.StatelessCartPole'>, 'num_gpus': 0, 'framework': 'torch', 'eager_tracing': False}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-17 09:55:19 (running for 00:00:31.45)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/1 GPUs, 0.0/41.89 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/condauser/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-17 09:55:24 (running for 00:00:36.46)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/1 GPUs, 0.0/41.89 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/condauser/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial PPO_StatelessCartPole_04a94_00000 reported episode_reward_max=148.0,episode_reward_min=12.0,episode_reward_mean=56.04,episode_len_mean=56.04,episode_media={},episodes_this_iter=68,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 0.1317059025188922, 'mean_inference_ms': 1.6450536233929083, 'mean_action_processing_ms': 0.05403977787226388, 'mean_env_wait_ms': 0.10505896508070886, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,timesteps_this_iter=0,agent_timesteps_total=16000,timers={'sample_time_ms': 7450.929, 'sample_throughput': 536.846, 'learn_time_ms': 4332.953, 'learn_throughput': 923.158, 'update_time_ms': 2.392},info={'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.19999999999999996, 'cur_lr': 5e-05, 'total_loss': 0.039460188167339025, 'policy_loss': -0.05475125294743162, 'vf_loss': 925.6147934422348, 'vf_explained_var': -0.026908190683885055, 'kl': 0.008249799579324282, 'entropy': 0.5992759823799133, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}}}, 'num_steps_sampled': 16000, 'num_agent_steps_sampled': 16000, 'num_steps_trained': 16000, 'num_steps_trained_this_iter': 0},perf={'cpu_util_percent': 18.775000000000002, 'ram_util_percent': 11.9} with parameters={'num_sgd_iter': 5, 'sgd_minibatch_size': 128, 'simple_optimizer': True, 'model': {'use_lstm': True, 'lstm_cell_size': 256, 'lstm_use_prev_action': True, 'lstm_use_prev_reward': True}, 'vf_loss_coeff': 0.0001, 'env': <class '__main__.StatelessCartPole'>, 'num_gpus': 0, 'framework': 'torch', 'eager_tracing': False}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-17 09:55:30 (running for 00:00:42.02)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/1 GPUs, 0.0/41.89 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/condauser/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial PPO_StatelessCartPole_04a94_00000 reported episode_reward_max=199.0,episode_reward_min=12.0,episode_reward_mean=69.0,episode_len_mean=69.0,episode_media={},episodes_this_iter=46,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 0.1302693939858247, 'mean_inference_ms': 1.6445540120709592, 'mean_action_processing_ms': 0.054189910292762614, 'mean_env_wait_ms': 0.10500728520623709, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,timesteps_this_iter=0,agent_timesteps_total=20000,timers={'sample_time_ms': 7656.826, 'sample_throughput': 522.41, 'learn_time_ms': 4328.171, 'learn_throughput': 924.178, 'update_time_ms': 2.403},info={'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.19999999999999996, 'cur_lr': 5e-05, 'total_loss': 0.07950396437310811, 'policy_loss': -0.03738198415799574, 'vf_loss': 1145.5631712942413, 'vf_explained_var': -0.1179043224363616, 'kl': 0.01164815315664603, 'entropy': 0.5869355700232766, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}}}, 'num_steps_sampled': 20000, 'num_agent_steps_sampled': 20000, 'num_steps_trained': 20000, 'num_steps_trained_this_iter': 0},perf={'cpu_util_percent': 18.666666666666664, 'ram_util_percent': 11.9} with parameters={'num_sgd_iter': 5, 'sgd_minibatch_size': 128, 'simple_optimizer': True, 'model': {'use_lstm': True, 'lstm_cell_size': 256, 'lstm_use_prev_action': True, 'lstm_use_prev_reward': True}, 'vf_loss_coeff': 0.0001, 'env': <class '__main__.StatelessCartPole'>, 'num_gpus': 0, 'framework': 'torch', 'eager_tracing': False}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-17 09:55:35 (running for 00:00:47.45)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/1 GPUs, 0.0/41.89 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/condauser/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-17 09:55:40 (running for 00:00:52.46)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/1 GPUs, 0.0/41.89 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/condauser/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial PPO_StatelessCartPole_04a94_00000 reported episode_reward_max=336.0,episode_reward_min=12.0,episode_reward_mean=65.33,episode_len_mean=65.33,episode_media={},episodes_this_iter=72,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 0.1290892645727551, 'mean_inference_ms': 1.65357717002127, 'mean_action_processing_ms': 0.05448484886964919, 'mean_env_wait_ms': 0.1053533642517326, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,timesteps_this_iter=0,agent_timesteps_total=24000,timers={'sample_time_ms': 7835.183, 'sample_throughput': 510.518, 'learn_time_ms': 4352.036, 'learn_throughput': 919.11, 'update_time_ms': 2.356},info={'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.19999999999999996, 'cur_lr': 5e-05, 'total_loss': 0.004171868046802102, 'policy_loss': -0.052059624863393376, 'vf_loss': 530.9887540875059, 'vf_explained_var': -0.2840189688133471, 'kl': 0.015663093091167663, 'entropy': 0.5844354914896416, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}}}, 'num_steps_sampled': 24000, 'num_agent_steps_sampled': 24000, 'num_steps_trained': 24000, 'num_steps_trained_this_iter': 0},perf={'cpu_util_percent': 18.50769230769231, 'ram_util_percent': 11.900000000000002} with parameters={'num_sgd_iter': 5, 'sgd_minibatch_size': 128, 'simple_optimizer': True, 'model': {'use_lstm': True, 'lstm_cell_size': 256, 'lstm_use_prev_action': True, 'lstm_use_prev_reward': True}, 'vf_loss_coeff': 0.0001, 'env': <class '__main__.StatelessCartPole'>, 'num_gpus': 0, 'framework': 'torch', 'eager_tracing': False}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-17 09:55:46 (running for 00:00:58.34)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/1 GPUs, 0.0/41.89 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/condauser/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-17 09:55:51 (running for 00:01:03.36)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/1 GPUs, 0.0/41.89 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/condauser/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial PPO_StatelessCartPole_04a94_00000 reported episode_reward_max=336.0,episode_reward_min=16.0,episode_reward_mean=74.72,episode_len_mean=74.72,episode_media={},episodes_this_iter=43,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 0.1284386966710144, 'mean_inference_ms': 1.6555806141618914, 'mean_action_processing_ms': 0.05453696507317079, 'mean_env_wait_ms': 0.10542598843986205, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,timesteps_this_iter=0,agent_timesteps_total=28000,timers={'sample_time_ms': 7948.928, 'sample_throughput': 503.213, 'learn_time_ms': 4337.746, 'learn_throughput': 922.138, 'update_time_ms': 2.357},info={'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.19999999999999996, 'cur_lr': 5e-05, 'total_loss': 0.0437585218273329, 'policy_loss': -0.0440175752635255, 'vf_loss': 855.2423182631984, 'vf_explained_var': -0.15156361334251636, 'kl': 0.011259329388127477, 'entropy': 0.5623696235093203, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}}}, 'num_steps_sampled': 28000, 'num_agent_steps_sampled': 28000, 'num_steps_trained': 28000, 'num_steps_trained_this_iter': 0},perf={'cpu_util_percent': 18.6, 'ram_util_percent': 11.9} with parameters={'num_sgd_iter': 5, 'sgd_minibatch_size': 128, 'simple_optimizer': True, 'model': {'use_lstm': True, 'lstm_cell_size': 256, 'lstm_use_prev_action': True, 'lstm_use_prev_reward': True}, 'vf_loss_coeff': 0.0001, 'env': <class '__main__.StatelessCartPole'>, 'num_gpus': 0, 'framework': 'torch', 'eager_tracing': False}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-17 09:55:56 (running for 00:01:08.75)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/1 GPUs, 0.0/41.89 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/condauser/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial PPO_StatelessCartPole_04a94_00000 reported episode_reward_max=298.0,episode_reward_min=14.0,episode_reward_mean=92.42,episode_len_mean=92.42,episode_media={},episodes_this_iter=38,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 0.12741076972783394, 'mean_inference_ms': 1.6530452251922276, 'mean_action_processing_ms': 0.05443674477484267, 'mean_env_wait_ms': 0.10531209295626351, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,timesteps_this_iter=0,agent_timesteps_total=32000,timers={'sample_time_ms': 8000.448, 'sample_throughput': 499.972, 'learn_time_ms': 4323.071, 'learn_throughput': 925.268, 'update_time_ms': 2.333},info={'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.19999999999999996, 'cur_lr': 5e-05, 'total_loss': 0.05420476183521025, 'policy_loss': -0.02725828059130546, 'vf_loss': 799.5932944557884, 'vf_explained_var': -0.1331702774221247, 'kl': 0.0075185658559445735, 'entropy': 0.5567249081351541, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}}}, 'num_steps_sampled': 32000, 'num_agent_steps_sampled': 32000, 'num_steps_trained': 32000, 'num_steps_trained_this_iter': 0},perf={'cpu_util_percent': 18.55833333333333, 'ram_util_percent': 11.9} with parameters={'num_sgd_iter': 5, 'sgd_minibatch_size': 128, 'simple_optimizer': True, 'model': {'use_lstm': True, 'lstm_cell_size': 256, 'lstm_use_prev_action': True, 'lstm_use_prev_reward': True}, 'vf_loss_coeff': 0.0001, 'env': <class '__main__.StatelessCartPole'>, 'num_gpus': 0, 'framework': 'torch', 'eager_tracing': False}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-17 09:56:02 (running for 00:01:14.09)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/1 GPUs, 0.0/41.89 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/condauser/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-17 09:56:07 (running for 00:01:19.10)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/1 GPUs, 0.0/41.89 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/condauser/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial PPO_StatelessCartPole_04a94_00000 reported episode_reward_max=298.0,episode_reward_min=14.0,episode_reward_mean=107.39,episode_len_mean=107.39,episode_media={},episodes_this_iter=29,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 0.12659400104925278, 'mean_inference_ms': 1.651657456045081, 'mean_action_processing_ms': 0.05444165264284477, 'mean_env_wait_ms': 0.10529071374671667, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,timesteps_this_iter=0,agent_timesteps_total=36000,timers={'sample_time_ms': 8039.136, 'sample_throughput': 497.566, 'learn_time_ms': 4321.802, 'learn_throughput': 925.54, 'update_time_ms': 2.309},info={'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.19999999999999996, 'cur_lr': 5e-05, 'total_loss': 0.07291675565594977, 'policy_loss': -0.028507097409755894, 'vf_loss': 1002.4483799789891, 'vf_explained_var': -0.09191193544503415, 'kl': 0.005895087228416709, 'entropy': 0.5383749423605023, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}}}, 'num_steps_sampled': 36000, 'num_agent_steps_sampled': 36000, 'num_steps_trained': 36000, 'num_steps_trained_this_iter': 0},perf={'cpu_util_percent': 18.466666666666672, 'ram_util_percent': 11.9} with parameters={'num_sgd_iter': 5, 'sgd_minibatch_size': 128, 'simple_optimizer': True, 'model': {'use_lstm': True, 'lstm_cell_size': 256, 'lstm_use_prev_action': True, 'lstm_use_prev_reward': True}, 'vf_loss_coeff': 0.0001, 'env': <class '__main__.StatelessCartPole'>, 'num_gpus': 0, 'framework': 'torch', 'eager_tracing': False}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-17 09:56:12 (running for 00:01:24.55)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/1 GPUs, 0.0/41.89 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/condauser/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial PPO_StatelessCartPole_04a94_00000 reported episode_reward_max=311.0,episode_reward_min=14.0,episode_reward_mean=126.61,episode_len_mean=126.61,episode_media={},episodes_this_iter=24,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 0.1258227305048867, 'mean_inference_ms': 1.6504020957778869, 'mean_action_processing_ms': 0.05438220198486384, 'mean_env_wait_ms': 0.10526007608929788, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,timesteps_this_iter=0,agent_timesteps_total=40000,timers={'sample_time_ms': 8080.301, 'sample_throughput': 495.031, 'learn_time_ms': 4334.94, 'learn_throughput': 922.735, 'update_time_ms': 2.288},info={'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.19999999999999996, 'cur_lr': 5e-05, 'total_loss': 0.07479058060082881, 'policy_loss': -0.02539817564846838, 'vf_loss': 983.9349574369543, 'vf_explained_var': -0.10829007765826057, 'kl': 0.00897632812242811, 'entropy': 0.5368540904101203, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}}}, 'num_steps_sampled': 40000, 'num_agent_steps_sampled': 40000, 'num_steps_trained': 40000, 'num_steps_trained_this_iter': 0},perf={'cpu_util_percent': 18.8, 'ram_util_percent': 11.9} with parameters={'num_sgd_iter': 5, 'sgd_minibatch_size': 128, 'simple_optimizer': True, 'model': {'use_lstm': True, 'lstm_cell_size': 256, 'lstm_use_prev_action': True, 'lstm_use_prev_reward': True}, 'vf_loss_coeff': 0.0001, 'env': <class '__main__.StatelessCartPole'>, 'num_gpus': 0, 'framework': 'torch', 'eager_tracing': False}. This trial completed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-17 09:56:17 (running for 00:01:29.23)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/12 CPUs, 0/1 GPUs, 0.0/41.89 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/condauser/ray_results/PPO<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_StatelessCartPole_04a94_00000</td><td>TERMINATED</td><td>172.31.0.4:6926</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         84.8593</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">  126.61</td><td style=\"text-align: right;\">                 311</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">            126.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=6916)\u001b[0m 2021-11-17 09:56:17,537\tERROR worker.py:425 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=6916)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=6916)\u001b[0m   File \"python/ray/_raylet.pyx\", line 558, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=6916)\u001b[0m   File \"python/ray/_raylet.pyx\", line 565, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=6916)\u001b[0m   File \"python/ray/_raylet.pyx\", line 569, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=6916)\u001b[0m   File \"python/ray/_raylet.pyx\", line 519, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=6916)\u001b[0m   File \"/opt/conda/lib/python3.9/site-packages/ray/_private/function_manager.py\", line 576, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=6916)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=6916)\u001b[0m   File \"/opt/conda/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 451, in _resume_span\n",
      "\u001b[2m\u001b[36m(pid=6916)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(pid=6916)\u001b[0m   File \"/opt/conda/lib/python3.9/site-packages/ray/actor.py\", line 1047, in __ray_terminate__\n",
      "\u001b[2m\u001b[36m(pid=6916)\u001b[0m     ray.actor.exit_actor()\n",
      "\u001b[2m\u001b[36m(pid=6916)\u001b[0m   File \"/opt/conda/lib/python3.9/site-packages/ray/actor.py\", line 1123, in exit_actor\n",
      "\u001b[2m\u001b[36m(pid=6916)\u001b[0m     raise exit\n",
      "\u001b[2m\u001b[36m(pid=6916)\u001b[0m SystemExit: 0\n",
      "\u001b[2m\u001b[36m(pid=6916)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6916)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=6916)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6916)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=6916)\u001b[0m   File \"python/ray/_raylet.pyx\", line 692, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=6916)\u001b[0m   File \"python/ray/_raylet.pyx\", line 521, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=6916)\u001b[0m   File \"python/ray/includes/libcoreworker.pxi\", line 33, in ray._raylet.ProfileEvent.__exit__\n",
      "\u001b[2m\u001b[36m(pid=6916)\u001b[0m   File \"/opt/conda/lib/python3.9/traceback.py\", line 167, in format_exc\n",
      "\u001b[2m\u001b[36m(pid=6916)\u001b[0m     return \"\".join(format_exception(*sys.exc_info(), limit=limit, chain=chain))\n",
      "\u001b[2m\u001b[36m(pid=6916)\u001b[0m   File \"/opt/conda/lib/python3.9/traceback.py\", line 120, in format_exception\n",
      "\u001b[2m\u001b[36m(pid=6916)\u001b[0m     return list(TracebackException(\n",
      "\u001b[2m\u001b[36m(pid=6916)\u001b[0m   File \"/opt/conda/lib/python3.9/traceback.py\", line 508, in __init__\n",
      "\u001b[2m\u001b[36m(pid=6916)\u001b[0m     self.stack = StackSummary.extract(\n",
      "\u001b[2m\u001b[36m(pid=6916)\u001b[0m   File \"/opt/conda/lib/python3.9/traceback.py\", line 366, in extract\n",
      "\u001b[2m\u001b[36m(pid=6916)\u001b[0m     f.line\n",
      "\u001b[2m\u001b[36m(pid=6916)\u001b[0m   File \"/opt/conda/lib/python3.9/traceback.py\", line 288, in line\n",
      "\u001b[2m\u001b[36m(pid=6916)\u001b[0m     self._line = linecache.getline(self.filename, self.lineno).strip()\n",
      "\u001b[2m\u001b[36m(pid=6916)\u001b[0m   File \"/opt/conda/lib/python3.9/linecache.py\", line 30, in getline\n",
      "\u001b[2m\u001b[36m(pid=6916)\u001b[0m     lines = getlines(filename, module_globals)\n",
      "\u001b[2m\u001b[36m(pid=6916)\u001b[0m   File \"/opt/conda/lib/python3.9/linecache.py\", line 43, in getlines\n",
      "\u001b[2m\u001b[36m(pid=6916)\u001b[0m     return cache[filename][2]\n",
      "\u001b[2m\u001b[36m(pid=6916)\u001b[0m   File \"/opt/conda/lib/python3.9/site-packages/ray/worker.py\", line 422, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=6916)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=6916)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[36m(pid=6916)\u001b[0m [2021-11-17 09:56:17,539 E 6916 7158] raylet_client.cc:159: IOError: Broken pipe [RayletClient] Failed to disconnect from raylet.\n",
      "\u001b[2m\u001b[36m(pid=6920)\u001b[0m 2021-11-17 09:56:17,537\tERROR worker.py:425 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=6920)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=6920)\u001b[0m   File \"python/ray/_raylet.pyx\", line 558, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=6920)\u001b[0m   File \"python/ray/_raylet.pyx\", line 565, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=6920)\u001b[0m   File \"python/ray/_raylet.pyx\", line 569, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=6920)\u001b[0m   File \"python/ray/_raylet.pyx\", line 519, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=6920)\u001b[0m   File \"/opt/conda/lib/python3.9/site-packages/ray/_private/function_manager.py\", line 576, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=6920)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=6920)\u001b[0m   File \"/opt/conda/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 451, in _resume_span\n",
      "\u001b[2m\u001b[36m(pid=6920)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(pid=6920)\u001b[0m   File \"/opt/conda/lib/python3.9/site-packages/ray/actor.py\", line 1047, in __ray_terminate__\n",
      "\u001b[2m\u001b[36m(pid=6920)\u001b[0m     ray.actor.exit_actor()\n",
      "\u001b[2m\u001b[36m(pid=6920)\u001b[0m   File \"/opt/conda/lib/python3.9/site-packages/ray/actor.py\", line 1123, in exit_actor\n",
      "\u001b[2m\u001b[36m(pid=6920)\u001b[0m     raise exit\n",
      "\u001b[2m\u001b[36m(pid=6920)\u001b[0m SystemExit: 0\n",
      "\u001b[2m\u001b[36m(pid=6920)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6920)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=6920)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6920)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=6920)\u001b[0m   File \"/opt/conda/lib/python3.9/linecache.py\", line 93, in updatecache\n",
      "\u001b[2m\u001b[36m(pid=6920)\u001b[0m     stat = os.stat(fullname)\n",
      "\u001b[2m\u001b[36m(pid=6920)\u001b[0m FileNotFoundError: [Errno 2] No such file or directory: 'python/ray/_raylet.pyx'\n",
      "\u001b[2m\u001b[36m(pid=6920)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6920)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=6920)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6920)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=6920)\u001b[0m   File \"python/ray/_raylet.pyx\", line 692, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=6920)\u001b[0m   File \"python/ray/_raylet.pyx\", line 521, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=6920)\u001b[0m   File \"python/ray/includes/libcoreworker.pxi\", line 33, in ray._raylet.ProfileEvent.__exit__\n",
      "\u001b[2m\u001b[36m(pid=6920)\u001b[0m   File \"/opt/conda/lib/python3.9/traceback.py\", line 167, in format_exc\n",
      "\u001b[2m\u001b[36m(pid=6920)\u001b[0m     return \"\".join(format_exception(*sys.exc_info(), limit=limit, chain=chain))\n",
      "\u001b[2m\u001b[36m(pid=6920)\u001b[0m   File \"/opt/conda/lib/python3.9/traceback.py\", line 120, in format_exception\n",
      "\u001b[2m\u001b[36m(pid=6920)\u001b[0m     return list(TracebackException(\n",
      "\u001b[2m\u001b[36m(pid=6920)\u001b[0m   File \"/opt/conda/lib/python3.9/traceback.py\", line 508, in __init__\n",
      "\u001b[2m\u001b[36m(pid=6920)\u001b[0m     self.stack = StackSummary.extract(\n",
      "\u001b[2m\u001b[36m(pid=6920)\u001b[0m   File \"/opt/conda/lib/python3.9/traceback.py\", line 366, in extract\n",
      "\u001b[2m\u001b[36m(pid=6920)\u001b[0m     f.line\n",
      "\u001b[2m\u001b[36m(pid=6920)\u001b[0m   File \"/opt/conda/lib/python3.9/traceback.py\", line 288, in line\n",
      "\u001b[2m\u001b[36m(pid=6920)\u001b[0m     self._line = linecache.getline(self.filename, self.lineno).strip()\n",
      "\u001b[2m\u001b[36m(pid=6920)\u001b[0m   File \"/opt/conda/lib/python3.9/linecache.py\", line 30, in getline\n",
      "\u001b[2m\u001b[36m(pid=6920)\u001b[0m     lines = getlines(filename, module_globals)\n",
      "\u001b[2m\u001b[36m(pid=6920)\u001b[0m   File \"/opt/conda/lib/python3.9/linecache.py\", line 46, in getlines\n",
      "\u001b[2m\u001b[36m(pid=6920)\u001b[0m     return updatecache(filename, module_globals)\n",
      "\u001b[2m\u001b[36m(pid=6920)\u001b[0m   File \"/opt/conda/lib/python3.9/linecache.py\", line 93, in updatecache\n",
      "\u001b[2m\u001b[36m(pid=6920)\u001b[0m     stat = os.stat(fullname)\n",
      "\u001b[2m\u001b[36m(pid=6920)\u001b[0m   File \"/opt/conda/lib/python3.9/site-packages/ray/worker.py\", line 422, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=6920)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=6920)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[36m(pid=6920)\u001b[0m [2021-11-17 09:56:17,541 E 6920 7166] raylet_client.cc:159: IOError: Broken pipe [RayletClient] Failed to disconnect from raylet.\n",
      "2021-11-17 09:56:17,643\tINFO tune.py:630 -- Total run time: 89.54 seconds (89.14 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "results = tune.run(args.run, config=config, stop=stop, verbose=2, checkpoint_at_end=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f4a620d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-17 09:56:44,810\tWARNING deprecation.py:38 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "2021-11-17 09:56:44,812\tINFO trainer.py:770 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=6921)\u001b[0m /opt/conda/lib/python3.9/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=6921)\u001b[0m   logger.warn(\n",
      "\u001b[2m\u001b[36m(pid=6923)\u001b[0m /opt/conda/lib/python3.9/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=6923)\u001b[0m   logger.warn(\n",
      "\u001b[2m\u001b[36m(pid=6923)\u001b[0m 2021-11-17 09:56:46,584\tWARNING deprecation.py:38 -- DeprecationWarning: `SampleBatch['is_training']` has been deprecated. Use `SampleBatch.is_training` instead. This will raise an error in the future!\n",
      "2021-11-17 09:56:46,700\tWARNING deprecation.py:38 -- DeprecationWarning: `SampleBatch['is_training']` has been deprecated. Use `SampleBatch.is_training` instead. This will raise an error in the future!\n",
      "2021-11-17 09:56:46,725\tWARNING trainer_template.py:185 -- `execution_plan` functions should accept `trainer`, `workers`, and `config` as args!\n",
      "2021-11-17 09:56:46,728\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n",
      "2021-11-17 09:56:46,767\tINFO trainable.py:416 -- Restored on 172.31.0.4 from checkpoint: /home/condauser/ray_results/PPO/PPO_StatelessCartPole_04a94_00000_0_2021-11-17_09-54-48/checkpoint_000010/checkpoint-10\n",
      "2021-11-17 09:56:46,769\tINFO trainable.py:424 -- Current state after restoring: {'_iteration': 10, '_timesteps_total': 0, '_time_total': 84.85933876037598, '_episodes_total': 729}\n",
      "/opt/conda/lib/python3.9/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode reward=128.0\n",
      "Episode reward=63.0\n",
      "Episode reward=77.0\n",
      "Episode reward=103.0\n",
      "Episode reward=50.0\n",
      "Episode reward=28.0\n",
      "Episode reward=214.0\n",
      "Episode reward=128.0\n",
      "Episode reward=190.0\n",
      "Episode reward=115.0\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "\n",
    "checkpoints = results.get_trial_checkpoints_paths(\n",
    "    trial=results.get_best_trial(\"episode_reward_mean\", mode=\"max\"),\n",
    "    metric=\"episode_reward_mean\")\n",
    "\n",
    "checkpoint_path = checkpoints[0][0]\n",
    "trainer = PPOTrainer(config)\n",
    "trainer.restore(checkpoint_path)\n",
    "\n",
    "# Inference loop.\n",
    "env = StatelessCartPole()\n",
    "obs = env.reset()\n",
    "# range(2) b/c h- and c-states of the LSTM.\n",
    "lstm_cell_size = 256\n",
    "init_state = state = [\n",
    "        np.zeros([lstm_cell_size], np.float32) for _ in range(2)\n",
    "]\n",
    "\n",
    "# Run manual inference loop for n episodes.\n",
    "for _ in range(10):\n",
    "    episode_reward = 0\n",
    "    reward = 0.0\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    state = init_state\n",
    "    prev_a = 1\n",
    "    prev_r = 0.0\n",
    "\n",
    "    while not done:\n",
    "        a, state_out, _ = trainer.compute_single_action(obs, state, prev_action=prev_a, prev_reward=prev_r)\n",
    "        obs, reward, done, _ = env.step(a)\n",
    "        episode_reward += reward\n",
    "        prev_a = a\n",
    "        prev_r = reward\n",
    "        state = state_out\n",
    "\n",
    "    print(f\"Episode reward={episode_reward}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3854e610",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
