{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de31423b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "from gym.utils import seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7b32a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatelessCartPoleMD(gym.Env):\n",
    "    \"\"\"Partially observable variant of the CartPole gym environment.\n",
    "\n",
    "    https://github.com/openai/gym/blob/master/gym/envs/classic_control/\n",
    "    cartpole.py\n",
    "\n",
    "    We delete the velocity component of the state, so that it can only be\n",
    "    solved by a LSTM policy.\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\n",
    "        \"render.modes\": [\"human\", \"rgb_array\"],\n",
    "        \"video.frames_per_second\": 60\n",
    "    }\n",
    "\n",
    "    def __init__(self, config=None):\n",
    "        self.gravity = 9.8\n",
    "        self.masscart = 1.0\n",
    "        self.masspole = 0.1\n",
    "        self.total_mass = (self.masspole + self.masscart)\n",
    "        self.length = 0.5  # actually half the pole's length\n",
    "        self.polemass_length = (self.masspole * self.length)\n",
    "        self.force_mag = 10.0\n",
    "        self.tau = 0.02  # seconds between state updates\n",
    "\n",
    "        # Angle at which to fail the episode\n",
    "        self.theta_threshold_radians = 12 * 2 * math.pi / 360\n",
    "        self.x_threshold = 2.4\n",
    "\n",
    "        high = np.array([\n",
    "            self.x_threshold * 2,\n",
    "            self.theta_threshold_radians * 2,\n",
    "        ])\n",
    "\n",
    "        self.action_space = spaces.MultiDiscrete([2,2])\n",
    "        self.observation_space = spaces.Box(-high, high)\n",
    "\n",
    "        self.seed()\n",
    "        self.viewer = None\n",
    "        self.state = None\n",
    "\n",
    "        self.steps_beyond_done = None\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(\n",
    "            action), \"%r (%s) invalid\" % (action, type(action))\n",
    "        action = action[0] and action[1]\n",
    "        state = self.state\n",
    "        x, x_dot, theta, theta_dot = state\n",
    "        force = self.force_mag if action == 1 else -self.force_mag\n",
    "        costheta = math.cos(theta)\n",
    "        sintheta = math.sin(theta)\n",
    "        temp = (force + self.polemass_length * theta_dot * theta_dot * sintheta\n",
    "                ) / self.total_mass\n",
    "        thetaacc = (self.gravity * sintheta - costheta * temp) / (\n",
    "            self.length *\n",
    "            (4.0 / 3.0 - self.masspole * costheta * costheta / self.total_mass)\n",
    "        )\n",
    "        xacc = (temp -\n",
    "                self.polemass_length * thetaacc * costheta / self.total_mass)\n",
    "        x = x + self.tau * x_dot\n",
    "        x_dot = x_dot + self.tau * xacc\n",
    "        theta = theta + self.tau * theta_dot\n",
    "        theta_dot = theta_dot + self.tau * thetaacc\n",
    "        self.state = (x, x_dot, theta, theta_dot)\n",
    "        done = (x < -self.x_threshold or x > self.x_threshold\n",
    "                or theta < -self.theta_threshold_radians\n",
    "                or theta > self.theta_threshold_radians)\n",
    "        done = bool(done)\n",
    "\n",
    "        if not done:\n",
    "            reward = 1.0\n",
    "        elif self.steps_beyond_done is None:\n",
    "            # Pole just fell!\n",
    "            self.steps_beyond_done = 0\n",
    "            reward = 1.0\n",
    "        else:\n",
    "            self.steps_beyond_done += 1\n",
    "            reward = 0.0\n",
    "\n",
    "        rv = np.r_[self.state[0], self.state[2]]\n",
    "        return rv, reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.np_random.uniform(low=-0.05, high=0.05, size=(4, ))\n",
    "        self.steps_beyond_done = None\n",
    "\n",
    "        rv = np.r_[self.state[0], self.state[2]]\n",
    "        return rv\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        screen_width = 600\n",
    "        screen_height = 400\n",
    "\n",
    "        world_width = self.x_threshold * 2\n",
    "        scale = screen_width / world_width\n",
    "        carty = 100  # TOP OF CART\n",
    "        polewidth = 10.0\n",
    "        polelen = scale * 1.0\n",
    "        cartwidth = 50.0\n",
    "        cartheight = 30.0\n",
    "\n",
    "        if self.viewer is None:\n",
    "            from gym.envs.classic_control import rendering\n",
    "            self.viewer = rendering.Viewer(screen_width, screen_height)\n",
    "            l, r, t, b = (-cartwidth / 2, cartwidth / 2, cartheight / 2,\n",
    "                          -cartheight / 2)\n",
    "            axleoffset = cartheight / 4.0\n",
    "            cart = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])\n",
    "            self.carttrans = rendering.Transform()\n",
    "            cart.add_attr(self.carttrans)\n",
    "            self.viewer.add_geom(cart)\n",
    "            l, r, t, b = (-polewidth / 2, polewidth / 2,\n",
    "                          polelen - polewidth / 2, -polewidth / 2)\n",
    "            pole = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])\n",
    "            pole.set_color(.8, .6, .4)\n",
    "            self.poletrans = rendering.Transform(translation=(0, axleoffset))\n",
    "            pole.add_attr(self.poletrans)\n",
    "            pole.add_attr(self.carttrans)\n",
    "            self.viewer.add_geom(pole)\n",
    "            self.axle = rendering.make_circle(polewidth / 2)\n",
    "            self.axle.add_attr(self.poletrans)\n",
    "            self.axle.add_attr(self.carttrans)\n",
    "            self.axle.set_color(.5, .5, .8)\n",
    "            self.viewer.add_geom(self.axle)\n",
    "            self.track = rendering.Line((0, carty), (screen_width, carty))\n",
    "            self.track.set_color(0, 0, 0)\n",
    "            self.viewer.add_geom(self.track)\n",
    "\n",
    "        if self.state is None:\n",
    "            return None\n",
    "\n",
    "        x = self.state\n",
    "        cartx = x[0] * scale + screen_width / 2.0  # MIDDLE OF CART\n",
    "        self.carttrans.set_translation(cartx, carty)\n",
    "        self.poletrans.set_rotation(-x[2])\n",
    "\n",
    "        return self.viewer.render(return_rgb_array=mode == \"rgb_array\")\n",
    "\n",
    "    def close(self):\n",
    "        if self.viewer:\n",
    "            self.viewer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d4c5a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "from ray.rllib.utils.test_utils import check_learning_achieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71f0c308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--stop-reward'], dest='stop_reward', nargs=None, const=None, default=150.0, type=<class 'float'>, choices=None, help='Reward at which we stop training.', metavar=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    \"--run\",\n",
    "    type=str,\n",
    "    default=\"PPO\",\n",
    "    help=\"The RLlib-registered algorithm to use.\")\n",
    "parser.add_argument(\"--num-cpus\", type=int, default=0)\n",
    "parser.add_argument(\n",
    "    \"--framework\",\n",
    "    choices=[\"tf\", \"tf2\", \"tfe\", \"torch\"],\n",
    "    default=\"torch\",\n",
    "    help=\"The DL framework specifier.\")\n",
    "parser.add_argument(\"--eager-tracing\", action=\"store_true\")\n",
    "parser.add_argument(\"--use-prev-action\", action=\"store_true\")\n",
    "parser.add_argument(\"--use-prev-reward\", action=\"store_true\")\n",
    "parser.add_argument(\n",
    "    \"--as-test\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Whether this script should be run as a test: --stop-reward must \"\n",
    "    \"be achieved within --stop-timesteps AND --stop-iters.\")\n",
    "parser.add_argument(\n",
    "    \"--stop-iters\",\n",
    "    type=int,\n",
    "    default=200,\n",
    "    help=\"Number of iterations to train.\")\n",
    "parser.add_argument(\n",
    "    \"--stop-timesteps\",\n",
    "    type=int,\n",
    "    default=100000,\n",
    "    help=\"Number of timesteps to train.\")\n",
    "parser.add_argument(\n",
    "    \"--stop-reward\",\n",
    "    type=float,\n",
    "    default=150.0,\n",
    "    help=\"Reward at which we stop training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "793cb424",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-17 09:49:55,090\tWARNING services.py:1748 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67059712 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=10.24gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "\n",
    "args = parser.parse_args(\"--stop-iters=10 --use-prev-action --use-prev-reward\".split())\n",
    "\n",
    "ray.init(num_cpus=args.num_cpus or None)\n",
    "\n",
    "configs = {\n",
    "    \"PPO\": {\n",
    "        \"num_sgd_iter\": 5,\n",
    "        \"sgd_minibatch_size\": 128,\n",
    "        \"simple_optimizer\": True,        \n",
    "        \"model\": {\n",
    "            \"vf_share_layers\": True,\n",
    "        },\n",
    "        \"vf_loss_coeff\": 0.0001,\n",
    "    },\n",
    "    \"IMPALA\": {\n",
    "        \"num_workers\": 2,\n",
    "        \"num_gpus\": 0,\n",
    "        \"vf_loss_coeff\": 0.01,\n",
    "    },\n",
    "}\n",
    "\n",
    "config = dict(\n",
    "    configs[args.run],\n",
    "    **{\n",
    "        \"env\": StatelessCartPoleMD,\n",
    "        # Use GPUs iff `RLLIB_NUM_GPUS` env var set to > 0.\n",
    "        \"num_gpus\": int(os.environ.get(\"RLLIB_NUM_GPUS\", \"0\")),\n",
    "        \"model\": {\n",
    "            \"use_lstm\": True,\n",
    "            \"lstm_cell_size\": 256,\n",
    "            \"lstm_use_prev_action\": args.use_prev_action,\n",
    "            \"lstm_use_prev_reward\": args.use_prev_reward,\n",
    "        },\n",
    "        \"framework\": args.framework,\n",
    "        # Run with tracing enabled for tfe/tf2?\n",
    "        \"eager_tracing\": args.eager_tracing,\n",
    "    })\n",
    "\n",
    "stop = {\n",
    "    \"training_iteration\": args.stop_iters,\n",
    "    \"timesteps_total\": args.stop_timesteps,\n",
    "    \"episode_reward_mean\": args.stop_reward,\n",
    "}\n",
    "\n",
    "# To run the Trainer without tune.run, using our LSTM model and\n",
    "# manual state-in handling, do the following:\n",
    "\n",
    "# Example (use `config` from the above code):\n",
    "# >> import numpy as np\n",
    "# >> from ray.rllib.agents.ppo import PPOTrainer\n",
    "# >>\n",
    "# >> trainer = PPOTrainer(config)\n",
    "# >> lstm_cell_size = config[\"model\"][\"lstm_cell_size\"]\n",
    "# >> env = StatelessCartPole()\n",
    "# >> obs = env.reset()\n",
    "# >>\n",
    "# >> # range(2) b/c h- and c-states of the LSTM.\n",
    "# >> init_state = state = [\n",
    "# ..     np.zeros([lstm_cell_size], np.float32) for _ in range(2)\n",
    "# .. ]\n",
    "# >> prev_a = 0\n",
    "# >> prev_r = 0.0\n",
    "# >>\n",
    "# >> while True:\n",
    "# >>     a, state_out, _ = trainer.compute_single_action(\n",
    "# ..         obs, state, prev_a, prev_r)\n",
    "# >>     obs, reward, done, _ = env.step(a)\n",
    "# >>     if done:\n",
    "# >>         obs = env.reset()\n",
    "# >>         state = init_state\n",
    "# >>         prev_a = 0\n",
    "# >>         prev_r = 0.0\n",
    "# >>     else:\n",
    "# >>         state = state_out\n",
    "# >>         prev_a = a\n",
    "# >>         prev_r = reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee618e48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_sgd_iter': 5,\n",
       " 'sgd_minibatch_size': 128,\n",
       " 'simple_optimizer': True,\n",
       " 'model': {'use_lstm': True,\n",
       "  'lstm_cell_size': 256,\n",
       "  'lstm_use_prev_action': True,\n",
       "  'lstm_use_prev_reward': True},\n",
       " 'vf_loss_coeff': 0.0001,\n",
       " 'env': __main__.StatelessCartPoleMD,\n",
       " 'num_gpus': 0,\n",
       " 'framework': 'torch',\n",
       " 'eager_tracing': False}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b904902a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-17 09:50:21,427\tERROR syncer.py:75 -- Log sync requires rsync to be installed.\n",
      "\u001b[2m\u001b[36m(pid=6081)\u001b[0m 2021-11-17 09:50:23,059\tWARNING deprecation.py:38 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=6081)\u001b[0m 2021-11-17 09:50:23,059\tINFO trainer.py:770 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=6086)\u001b[0m /opt/conda/lib/python3.9/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=6086)\u001b[0m   logger.warn(\n",
      "\u001b[2m\u001b[36m(pid=6081)\u001b[0m 2021-11-17 09:50:24,770\tWARNING deprecation.py:38 -- DeprecationWarning: `SampleBatch['is_training']` has been deprecated. Use `SampleBatch.is_training` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=6081)\u001b[0m 2021-11-17 09:50:24,791\tWARNING trainer_template.py:185 -- `execution_plan` functions should accept `trainer`, `workers`, and `config` as args!\n",
      "\u001b[2m\u001b[36m(pid=6086)\u001b[0m 2021-11-17 09:50:24,693\tWARNING deprecation.py:38 -- DeprecationWarning: `SampleBatch['is_training']` has been deprecated. Use `SampleBatch.is_training` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=6081)\u001b[0m 2021-11-17 09:50:24,792\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-17 09:50:24 (running for 00:00:03.65)<br>Memory usage on this node: 6.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/1 GPUs, 0.0/42.77 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/condauser/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=6088)\u001b[0m /opt/conda/lib/python3.9/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=6088)\u001b[0m   logger.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-17 09:50:26 (running for 00:00:05.66)<br>Memory usage on this node: 6.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/1 GPUs, 0.0/42.77 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/condauser/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=6081)\u001b[0m 2021-11-17 09:50:29,672\tWARNING deprecation.py:38 -- DeprecationWarning: `_get_slice_indices` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=6081)\u001b[0m 2021-11-17 09:50:29,676\tWARNING deprecation.py:38 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-17 09:50:31 (running for 00:00:10.68)<br>Memory usage on this node: 6.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/1 GPUs, 0.0/42.77 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/condauser/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial PPO_StatelessCartPoleMD_65802_00000 reported episode_reward_max=33.0,episode_reward_min=8.0,episode_reward_mean=13.537414965986395,episode_len_mean=13.537414965986395,episode_media={},episodes_this_iter=294,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 0.1510864571064395, 'mean_inference_ms': 1.8794677341242545, 'mean_action_processing_ms': 0.049682679641403514, 'mean_env_wait_ms': 0.11156463169791632, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,timesteps_this_iter=0,agent_timesteps_total=4000,timers={'sample_time_ms': 4879.835, 'sample_throughput': 819.7, 'learn_time_ms': 4524.774, 'learn_throughput': 884.022, 'update_time_ms': 5.0},info={'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.19999999999999998, 'cur_lr': 5.000000000000001e-05, 'total_loss': -0.05162774252821691, 'policy_loss': -0.06205586806172505, 'vf_loss': 77.98367164134979, 'vf_explained_var': -0.0014012034982442856, 'kl': 0.013148796992417139, 'entropy': 1.3687276363372802, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}}}, 'num_steps_sampled': 4000, 'num_agent_steps_sampled': 4000, 'num_steps_trained': 4000},perf={'cpu_util_percent': 16.192857142857143, 'ram_util_percent': 10.192857142857141} with parameters={'num_sgd_iter': 5, 'sgd_minibatch_size': 128, 'simple_optimizer': True, 'model': {'use_lstm': True, 'lstm_cell_size': 256, 'lstm_use_prev_action': True, 'lstm_use_prev_reward': True}, 'vf_loss_coeff': 0.0001, 'env': <class '__main__.StatelessCartPoleMD'>, 'num_gpus': 0, 'framework': 'torch', 'eager_tracing': False}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-17 09:50:37 (running for 00:00:16.12)<br>Memory usage on this node: 6.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/1 GPUs, 0.0/42.77 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/condauser/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-17 09:50:42 (running for 00:00:21.14)<br>Memory usage on this node: 6.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/1 GPUs, 0.0/42.77 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/condauser/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial PPO_StatelessCartPoleMD_65802_00000 reported episode_reward_max=67.0,episode_reward_min=8.0,episode_reward_mean=16.56611570247934,episode_len_mean=16.56611570247934,episode_media={},episodes_this_iter=242,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 0.14670172101926773, 'mean_inference_ms': 1.8799829329773303, 'mean_action_processing_ms': 0.049307438186076635, 'mean_env_wait_ms': 0.11051055798849536, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,timesteps_this_iter=0,agent_timesteps_total=8000,timers={'sample_time_ms': 7068.638, 'sample_throughput': 565.88, 'learn_time_ms': 4511.547, 'learn_throughput': 886.614, 'update_time_ms': 3.989},info={'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.19999999999999996, 'cur_lr': 5e-05, 'total_loss': -0.05222726471609238, 'policy_loss': -0.07017515576021238, 'vf_loss': 150.16884626908737, 'vf_explained_var': -0.004741010521397446, 'kl': 0.014655029477512283, 'entropy': 1.3165210514357595, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}}}, 'num_steps_sampled': 8000, 'num_agent_steps_sampled': 8000, 'num_steps_trained': 8000, 'num_steps_trained_this_iter': 0},perf={'cpu_util_percent': 16.56153846153846, 'ram_util_percent': 10.2} with parameters={'num_sgd_iter': 5, 'sgd_minibatch_size': 128, 'simple_optimizer': True, 'model': {'use_lstm': True, 'lstm_cell_size': 256, 'lstm_use_prev_action': True, 'lstm_use_prev_reward': True}, 'vf_loss_coeff': 0.0001, 'env': <class '__main__.StatelessCartPoleMD'>, 'num_gpus': 0, 'framework': 'torch', 'eager_tracing': False}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-17 09:50:47 (running for 00:00:26.35)<br>Memory usage on this node: 6.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/1 GPUs, 0.0/42.77 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/condauser/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-17 09:50:52 (running for 00:00:31.37)<br>Memory usage on this node: 6.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/1 GPUs, 0.0/42.77 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/condauser/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial PPO_StatelessCartPoleMD_65802_00000 reported episode_reward_max=64.0,episode_reward_min=8.0,episode_reward_mean=20.625,episode_len_mean=20.625,episode_media={},episodes_this_iter=192,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 0.14268371957292536, 'mean_inference_ms': 1.8834917131665085, 'mean_action_processing_ms': 0.04926111584523383, 'mean_env_wait_ms': 0.11045039560916271, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,timesteps_this_iter=0,agent_timesteps_total=12000,timers={'sample_time_ms': 7756.377, 'sample_throughput': 515.705, 'learn_time_ms': 4522.345, 'learn_throughput': 884.497, 'update_time_ms': 3.375},info={'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.19999999999999996, 'cur_lr': 5e-05, 'total_loss': -0.015369668642454076, 'policy_loss': -0.037946478370577096, 'vf_loss': 196.2572309551817, 'vf_explained_var': -0.006704347061388421, 'kl': 0.014755427536670892, 'entropy': 1.2346122069792314, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}}}, 'num_steps_sampled': 12000, 'num_agent_steps_sampled': 12000, 'num_steps_trained': 12000, 'num_steps_trained_this_iter': 0},perf={'cpu_util_percent': 16.669230769230772, 'ram_util_percent': 10.253846153846153} with parameters={'num_sgd_iter': 5, 'sgd_minibatch_size': 128, 'simple_optimizer': True, 'model': {'use_lstm': True, 'lstm_cell_size': 256, 'lstm_use_prev_action': True, 'lstm_use_prev_reward': True}, 'vf_loss_coeff': 0.0001, 'env': <class '__main__.StatelessCartPoleMD'>, 'num_gpus': 0, 'framework': 'torch', 'eager_tracing': False}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-17 09:50:57 (running for 00:00:36.51)<br>Memory usage on this node: 6.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/1 GPUs, 0.0/42.77 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/condauser/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial PPO_StatelessCartPoleMD_65802_00000 reported episode_reward_max=76.0,episode_reward_min=9.0,episode_reward_mean=24.74846625766871,episode_len_mean=24.74846625766871,episode_media={},episodes_this_iter=163,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 0.13999387460443846, 'mean_inference_ms': 1.8921175438316031, 'mean_action_processing_ms': 0.049409972891330194, 'mean_env_wait_ms': 0.11091243696397153, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,timesteps_this_iter=0,agent_timesteps_total=16000,timers={'sample_time_ms': 8118.676, 'sample_throughput': 492.691, 'learn_time_ms': 4490.377, 'learn_throughput': 890.794, 'update_time_ms': 3.079},info={'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.19999999999999998, 'cur_lr': 5.000000000000001e-05, 'total_loss': 0.0033017040404956788, 'policy_loss': -0.02532143609132618, 'vf_loss': 270.43356490135193, 'vf_explained_var': -0.008976449817419052, 'kl': 0.007898936791367772, 'entropy': 1.2161229237914086, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}}}, 'num_steps_sampled': 16000, 'num_agent_steps_sampled': 16000, 'num_steps_trained': 16000, 'num_steps_trained_this_iter': 0},perf={'cpu_util_percent': 16.93076923076923, 'ram_util_percent': 10.3} with parameters={'num_sgd_iter': 5, 'sgd_minibatch_size': 128, 'simple_optimizer': True, 'model': {'use_lstm': True, 'lstm_cell_size': 256, 'lstm_use_prev_action': True, 'lstm_use_prev_reward': True}, 'vf_loss_coeff': 0.0001, 'env': <class '__main__.StatelessCartPoleMD'>, 'num_gpus': 0, 'framework': 'torch', 'eager_tracing': False}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-17 09:51:02 (running for 00:00:41.57)<br>Memory usage on this node: 6.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/1 GPUs, 0.0/42.77 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/condauser/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-17 09:51:07 (running for 00:00:46.58)<br>Memory usage on this node: 6.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/1 GPUs, 0.0/42.77 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/condauser/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial PPO_StatelessCartPoleMD_65802_00000 reported episode_reward_max=97.0,episode_reward_min=9.0,episode_reward_mean=29.76865671641791,episode_len_mean=29.76865671641791,episode_media={},episodes_this_iter=134,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 0.13703803350515167, 'mean_inference_ms': 1.8877089351382854, 'mean_action_processing_ms': 0.04923409217000663, 'mean_env_wait_ms': 0.11046893977365568, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,timesteps_this_iter=0,agent_timesteps_total=20000,timers={'sample_time_ms': 8307.173, 'sample_throughput': 481.512, 'learn_time_ms': 4486.955, 'learn_throughput': 891.473, 'update_time_ms': 3.238},info={'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.19999999999999996, 'cur_lr': 5e-05, 'total_loss': -0.008974620524906751, 'policy_loss': -0.051114389466855564, 'vf_loss': 395.1804012876568, 'vf_explained_var': -0.014398043805902655, 'kl': 0.013108650649707421, 'entropy': 1.159098488634283, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}}}, 'num_steps_sampled': 20000, 'num_agent_steps_sampled': 20000, 'num_steps_trained': 20000, 'num_steps_trained_this_iter': 0},perf={'cpu_util_percent': 16.423076923076927, 'ram_util_percent': 10.3} with parameters={'num_sgd_iter': 5, 'sgd_minibatch_size': 128, 'simple_optimizer': True, 'model': {'use_lstm': True, 'lstm_cell_size': 256, 'lstm_use_prev_action': True, 'lstm_use_prev_reward': True}, 'vf_loss_coeff': 0.0001, 'env': <class '__main__.StatelessCartPoleMD'>, 'num_gpus': 0, 'framework': 'torch', 'eager_tracing': False}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-17 09:51:12 (running for 00:00:51.72)<br>Memory usage on this node: 6.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/1 GPUs, 0.0/42.77 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/condauser/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-17 09:51:17 (running for 00:00:56.73)<br>Memory usage on this node: 6.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/1 GPUs, 0.0/42.77 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/condauser/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial PPO_StatelessCartPoleMD_65802_00000 reported episode_reward_max=144.0,episode_reward_min=9.0,episode_reward_mean=40.75,episode_len_mean=40.75,episode_media={},episodes_this_iter=97,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 0.13471231496346509, 'mean_inference_ms': 1.8873239733723235, 'mean_action_processing_ms': 0.04920399700085691, 'mean_env_wait_ms': 0.11044715433832804, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,timesteps_this_iter=0,agent_timesteps_total=24000,timers={'sample_time_ms': 8441.331, 'sample_throughput': 473.859, 'learn_time_ms': 4449.882, 'learn_throughput': 898.9, 'update_time_ms': 3.064},info={'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.19999999999999996, 'cur_lr': 5e-05, 'total_loss': 0.04466253703629429, 'policy_loss': -0.020575117017848022, 'vf_loss': 630.1404970111269, 'vf_explained_var': -0.019658603812708998, 'kl': 0.011118021588483165, 'entropy': 1.1159736958417026, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}}}, 'num_steps_sampled': 24000, 'num_agent_steps_sampled': 24000, 'num_steps_trained': 24000, 'num_steps_trained_this_iter': 0},perf={'cpu_util_percent': 16.869230769230768, 'ram_util_percent': 10.3} with parameters={'num_sgd_iter': 5, 'sgd_minibatch_size': 128, 'simple_optimizer': True, 'model': {'use_lstm': True, 'lstm_cell_size': 256, 'lstm_use_prev_action': True, 'lstm_use_prev_reward': True}, 'vf_loss_coeff': 0.0001, 'env': <class '__main__.StatelessCartPoleMD'>, 'num_gpus': 0, 'framework': 'torch', 'eager_tracing': False}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-17 09:51:23 (running for 00:01:02.63)<br>Memory usage on this node: 6.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/1 GPUs, 0.0/42.77 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/condauser/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial PPO_StatelessCartPoleMD_65802_00000 reported episode_reward_max=144.0,episode_reward_min=12.0,episode_reward_mean=48.11,episode_len_mean=48.11,episode_media={},episodes_this_iter=83,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 0.13299270319244258, 'mean_inference_ms': 1.8887730514155694, 'mean_action_processing_ms': 0.04922783111264435, 'mean_env_wait_ms': 0.11069827729459215, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,timesteps_this_iter=0,agent_timesteps_total=28000,timers={'sample_time_ms': 8527.438, 'sample_throughput': 469.074, 'learn_time_ms': 4424.18, 'learn_throughput': 904.122, 'update_time_ms': 2.942},info={'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.19999999999999996, 'cur_lr': 5e-05, 'total_loss': 0.02994333216638276, 'policy_loss': -0.029023381813683292, 'vf_loss': 568.6106243711529, 'vf_explained_var': -0.03515350638013898, 'kl': 0.010528255823673545, 'entropy': 1.109747470147682, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}}}, 'num_steps_sampled': 28000, 'num_agent_steps_sampled': 28000, 'num_steps_trained': 28000, 'num_steps_trained_this_iter': 0},perf={'cpu_util_percent': 16.76923076923077, 'ram_util_percent': 10.3} with parameters={'num_sgd_iter': 5, 'sgd_minibatch_size': 128, 'simple_optimizer': True, 'model': {'use_lstm': True, 'lstm_cell_size': 256, 'lstm_use_prev_action': True, 'lstm_use_prev_reward': True}, 'vf_loss_coeff': 0.0001, 'env': <class '__main__.StatelessCartPoleMD'>, 'num_gpus': 0, 'framework': 'torch', 'eager_tracing': False}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-17 09:51:28 (running for 00:01:07.66)<br>Memory usage on this node: 6.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/1 GPUs, 0.0/42.77 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/condauser/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-17 09:51:33 (running for 00:01:12.68)<br>Memory usage on this node: 6.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/1 GPUs, 0.0/42.77 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/condauser/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial PPO_StatelessCartPoleMD_65802_00000 reported episode_reward_max=154.0,episode_reward_min=12.0,episode_reward_mean=58.3,episode_len_mean=58.3,episode_media={},episodes_this_iter=60,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 0.13174432623531035, 'mean_inference_ms': 1.8937252752823808, 'mean_action_processing_ms': 0.04933836038555456, 'mean_env_wait_ms': 0.111220401813198, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,timesteps_this_iter=0,agent_timesteps_total=32000,timers={'sample_time_ms': 8599.616, 'sample_throughput': 465.137, 'learn_time_ms': 4407.203, 'learn_throughput': 907.605, 'update_time_ms': 2.851},info={'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.19999999999999996, 'cur_lr': 5e-05, 'total_loss': 0.04629507146098397, 'policy_loss': -0.028992511151414928, 'vf_loss': 729.1446302009351, 'vf_explained_var': -0.047029986887267144, 'kl': 0.011865613950903565, 'entropy': 1.1324538577686656, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}}}, 'num_steps_sampled': 32000, 'num_agent_steps_sampled': 32000, 'num_steps_trained': 32000, 'num_steps_trained_this_iter': 0},perf={'cpu_util_percent': 16.96153846153846, 'ram_util_percent': 10.3} with parameters={'num_sgd_iter': 5, 'sgd_minibatch_size': 128, 'simple_optimizer': True, 'model': {'use_lstm': True, 'lstm_cell_size': 256, 'lstm_use_prev_action': True, 'lstm_use_prev_reward': True}, 'vf_loss_coeff': 0.0001, 'env': <class '__main__.StatelessCartPoleMD'>, 'num_gpus': 0, 'framework': 'torch', 'eager_tracing': False}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-17 09:51:38 (running for 00:01:17.78)<br>Memory usage on this node: 6.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/1 GPUs, 0.0/42.77 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/condauser/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-17 09:51:43 (running for 00:01:22.80)<br>Memory usage on this node: 6.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/1 GPUs, 0.0/42.77 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/condauser/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial PPO_StatelessCartPoleMD_65802_00000 reported episode_reward_max=297.0,episode_reward_min=12.0,episode_reward_mean=73.41,episode_len_mean=73.41,episode_media={},episodes_this_iter=51,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 0.1304204839260088, 'mean_inference_ms': 1.8988378411288358, 'mean_action_processing_ms': 0.049493761600505476, 'mean_env_wait_ms': 0.11135842936240536, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,timesteps_this_iter=0,agent_timesteps_total=36000,timers={'sample_time_ms': 8637.324, 'sample_throughput': 463.106, 'learn_time_ms': 4398.633, 'learn_throughput': 909.374, 'update_time_ms': 2.773},info={'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.19999999999999996, 'cur_lr': 5e-05, 'total_loss': 0.07521230905937652, 'policy_loss': 5.750938346891692e-05, 'vf_loss': 732.8086897416548, 'vf_explained_var': -0.0030537973750721327, 'kl': 0.009369643598852272, 'entropy': 1.088766729109215, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}}}, 'num_steps_sampled': 36000, 'num_agent_steps_sampled': 36000, 'num_steps_trained': 36000, 'num_steps_trained_this_iter': 0},perf={'cpu_util_percent': 16.861538461538462, 'ram_util_percent': 10.3} with parameters={'num_sgd_iter': 5, 'sgd_minibatch_size': 128, 'simple_optimizer': True, 'model': {'use_lstm': True, 'lstm_cell_size': 256, 'lstm_use_prev_action': True, 'lstm_use_prev_reward': True}, 'vf_loss_coeff': 0.0001, 'env': <class '__main__.StatelessCartPoleMD'>, 'num_gpus': 0, 'framework': 'torch', 'eager_tracing': False}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-17 09:51:49 (running for 00:01:28.76)<br>Memory usage on this node: 6.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/1 GPUs, 0.0/42.77 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/condauser/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-17 09:51:54 (running for 00:01:33.78)<br>Memory usage on this node: 6.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/1 GPUs, 0.0/42.77 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/condauser/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial PPO_StatelessCartPoleMD_65802_00000 reported episode_reward_max=297.0,episode_reward_min=12.0,episode_reward_mean=81.01,episode_len_mean=81.01,episode_media={},episodes_this_iter=48,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 0.1289778364205617, 'mean_inference_ms': 1.8984699932914149, 'mean_action_processing_ms': 0.04946774626594633, 'mean_env_wait_ms': 0.11132797302088634, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,timesteps_this_iter=0,agent_timesteps_total=40000,timers={'sample_time_ms': 8672.253, 'sample_throughput': 461.241, 'learn_time_ms': 4401.632, 'learn_throughput': 908.754, 'update_time_ms': 2.724},info={'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.19999999999999996, 'cur_lr': 5e-05, 'total_loss': 0.05586183291831703, 'policy_loss': -0.025013950122802546, 'vf_loss': 791.1370985551314, 'vf_explained_var': -0.08231441830143785, 'kl': 0.00881036538726748, 'entropy': 1.0613125183365562, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}}}, 'num_steps_sampled': 40000, 'num_agent_steps_sampled': 40000, 'num_steps_trained': 40000, 'num_steps_trained_this_iter': 0},perf={'cpu_util_percent': 16.576923076923073, 'ram_util_percent': 10.3} with parameters={'num_sgd_iter': 5, 'sgd_minibatch_size': 128, 'simple_optimizer': True, 'model': {'use_lstm': True, 'lstm_cell_size': 256, 'lstm_use_prev_action': True, 'lstm_use_prev_reward': True}, 'vf_loss_coeff': 0.0001, 'env': <class '__main__.StatelessCartPoleMD'>, 'num_gpus': 0, 'framework': 'torch', 'eager_tracing': False}. This trial completed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-17 09:51:56 (running for 00:01:34.97)<br>Memory usage on this node: 6.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/12 CPUs, 0/1 GPUs, 0.0/42.77 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/condauser/ray_results/PPO<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_StatelessCartPoleMD_65802_00000</td><td>TERMINATED</td><td>172.31.0.4:6081</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         90.7405</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">   81.01</td><td style=\"text-align: right;\">                 297</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">             81.01</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=6086)\u001b[0m 2021-11-17 09:51:56,655\tERROR worker.py:425 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=6086)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=6086)\u001b[0m   File \"python/ray/_raylet.pyx\", line 558, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=6086)\u001b[0m   File \"python/ray/_raylet.pyx\", line 565, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=6086)\u001b[0m   File \"python/ray/_raylet.pyx\", line 569, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=6086)\u001b[0m   File \"python/ray/_raylet.pyx\", line 519, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=6086)\u001b[0m   File \"/opt/conda/lib/python3.9/site-packages/ray/_private/function_manager.py\", line 576, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=6086)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=6086)\u001b[0m   File \"/opt/conda/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 451, in _resume_span\n",
      "\u001b[2m\u001b[36m(pid=6086)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(pid=6086)\u001b[0m   File \"/opt/conda/lib/python3.9/site-packages/ray/actor.py\", line 1047, in __ray_terminate__\n",
      "\u001b[2m\u001b[36m(pid=6086)\u001b[0m     ray.actor.exit_actor()\n",
      "\u001b[2m\u001b[36m(pid=6086)\u001b[0m   File \"/opt/conda/lib/python3.9/site-packages/ray/actor.py\", line 1123, in exit_actor\n",
      "\u001b[2m\u001b[36m(pid=6086)\u001b[0m     raise exit\n",
      "\u001b[2m\u001b[36m(pid=6086)\u001b[0m SystemExit: 0\n",
      "\u001b[2m\u001b[36m(pid=6086)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6086)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=6086)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6086)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=6086)\u001b[0m   File \"python/ray/_raylet.pyx\", line 692, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=6086)\u001b[0m   File \"python/ray/_raylet.pyx\", line 521, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=6086)\u001b[0m   File \"python/ray/includes/libcoreworker.pxi\", line 42, in ray._raylet.ProfileEvent.__exit__\n",
      "\u001b[2m\u001b[36m(pid=6086)\u001b[0m   File \"/opt/conda/lib/python3.9/json/__init__.py\", line 227, in dumps\n",
      "\u001b[2m\u001b[36m(pid=6086)\u001b[0m     if (not skipkeys and ensure_ascii and\n",
      "\u001b[2m\u001b[36m(pid=6086)\u001b[0m   File \"/opt/conda/lib/python3.9/site-packages/ray/worker.py\", line 422, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=6086)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=6086)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[36m(pid=6086)\u001b[0m [2021-11-17 09:51:56,658 E 6086 6330] raylet_client.cc:159: IOError: Broken pipe [RayletClient] Failed to disconnect from raylet.\n",
      "\u001b[2m\u001b[36m(pid=6088)\u001b[0m 2021-11-17 09:51:56,655\tERROR worker.py:425 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=6088)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=6088)\u001b[0m   File \"python/ray/_raylet.pyx\", line 558, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=6088)\u001b[0m   File \"python/ray/_raylet.pyx\", line 565, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=6088)\u001b[0m   File \"python/ray/_raylet.pyx\", line 569, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=6088)\u001b[0m   File \"python/ray/_raylet.pyx\", line 519, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=6088)\u001b[0m   File \"/opt/conda/lib/python3.9/site-packages/ray/_private/function_manager.py\", line 576, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=6088)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=6088)\u001b[0m   File \"/opt/conda/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 451, in _resume_span\n",
      "\u001b[2m\u001b[36m(pid=6088)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(pid=6088)\u001b[0m   File \"/opt/conda/lib/python3.9/site-packages/ray/actor.py\", line 1047, in __ray_terminate__\n",
      "\u001b[2m\u001b[36m(pid=6088)\u001b[0m     ray.actor.exit_actor()\n",
      "\u001b[2m\u001b[36m(pid=6088)\u001b[0m   File \"/opt/conda/lib/python3.9/site-packages/ray/actor.py\", line 1123, in exit_actor\n",
      "\u001b[2m\u001b[36m(pid=6088)\u001b[0m     raise exit\n",
      "\u001b[2m\u001b[36m(pid=6088)\u001b[0m SystemExit: 0\n",
      "\u001b[2m\u001b[36m(pid=6088)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6088)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=6088)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6088)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=6088)\u001b[0m   File \"python/ray/_raylet.pyx\", line 692, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=6088)\u001b[0m   File \"python/ray/_raylet.pyx\", line 521, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=6088)\u001b[0m   File \"python/ray/includes/libcoreworker.pxi\", line 33, in ray._raylet.ProfileEvent.__exit__\n",
      "\u001b[2m\u001b[36m(pid=6088)\u001b[0m   File \"/opt/conda/lib/python3.9/traceback.py\", line 167, in format_exc\n",
      "\u001b[2m\u001b[36m(pid=6088)\u001b[0m     return \"\".join(format_exception(*sys.exc_info(), limit=limit, chain=chain))\n",
      "\u001b[2m\u001b[36m(pid=6088)\u001b[0m   File \"/opt/conda/lib/python3.9/traceback.py\", line 120, in format_exception\n",
      "\u001b[2m\u001b[36m(pid=6088)\u001b[0m     return list(TracebackException(\n",
      "\u001b[2m\u001b[36m(pid=6088)\u001b[0m   File \"/opt/conda/lib/python3.9/traceback.py\", line 508, in __init__\n",
      "\u001b[2m\u001b[36m(pid=6088)\u001b[0m     self.stack = StackSummary.extract(\n",
      "\u001b[2m\u001b[36m(pid=6088)\u001b[0m   File \"/opt/conda/lib/python3.9/traceback.py\", line 366, in extract\n",
      "\u001b[2m\u001b[36m(pid=6088)\u001b[0m     f.line\n",
      "\u001b[2m\u001b[36m(pid=6088)\u001b[0m   File \"/opt/conda/lib/python3.9/traceback.py\", line 288, in line\n",
      "\u001b[2m\u001b[36m(pid=6088)\u001b[0m     self._line = linecache.getline(self.filename, self.lineno).strip()\n",
      "\u001b[2m\u001b[36m(pid=6088)\u001b[0m   File \"/opt/conda/lib/python3.9/linecache.py\", line 30, in getline\n",
      "\u001b[2m\u001b[36m(pid=6088)\u001b[0m     lines = getlines(filename, module_globals)\n",
      "\u001b[2m\u001b[36m(pid=6088)\u001b[0m   File \"/opt/conda/lib/python3.9/linecache.py\", line 46, in getlines\n",
      "\u001b[2m\u001b[36m(pid=6088)\u001b[0m     return updatecache(filename, module_globals)\n",
      "\u001b[2m\u001b[36m(pid=6088)\u001b[0m   File \"/opt/conda/lib/python3.9/linecache.py\", line 108, in updatecache\n",
      "\u001b[2m\u001b[36m(pid=6088)\u001b[0m     return []\n",
      "\u001b[2m\u001b[36m(pid=6088)\u001b[0m   File \"/opt/conda/lib/python3.9/site-packages/ray/worker.py\", line 422, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=6088)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=6088)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[36m(pid=6088)\u001b[0m [2021-11-17 09:51:56,659 E 6088 6322] raylet_client.cc:159: IOError: Broken pipe [RayletClient] Failed to disconnect from raylet.\n",
      "2021-11-17 09:51:56,761\tINFO tune.py:630 -- Total run time: 95.69 seconds (94.88 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "results = tune.run(args.run, config=config, stop=stop, verbose=2, checkpoint_at_end=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f4a620d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-17 09:57:31,166\tWARNING deprecation.py:38 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=6085)\u001b[0m /opt/conda/lib/python3.9/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=6085)\u001b[0m   logger.warn(\n",
      "\u001b[2m\u001b[36m(pid=6084)\u001b[0m /opt/conda/lib/python3.9/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=6084)\u001b[0m   logger.warn(\n",
      "\u001b[2m\u001b[36m(pid=6084)\u001b[0m 2021-11-17 09:57:33,250\tWARNING deprecation.py:38 -- DeprecationWarning: `SampleBatch['is_training']` has been deprecated. Use `SampleBatch.is_training` instead. This will raise an error in the future!\n",
      "2021-11-17 09:57:33,323\tWARNING trainer_template.py:185 -- `execution_plan` functions should accept `trainer`, `workers`, and `config` as args!\n",
      "2021-11-17 09:57:33,325\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n",
      "2021-11-17 09:57:33,363\tINFO trainable.py:416 -- Restored on 172.31.0.4 from checkpoint: /home/condauser/ray_results/PPO/PPO_StatelessCartPoleMD_65802_00000_0_2021-11-17_09-50-21/checkpoint_000010/checkpoint-10\n",
      "2021-11-17 09:57:33,365\tINFO trainable.py:424 -- Current state after restoring: {'_iteration': 10, '_timesteps_total': 0, '_time_total': 90.74048352241516, '_episodes_total': 1364}\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for dimension 1 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5980/2835217490.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_single_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprev_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_reward\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprev_r\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mepisode_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/ray/rllib/agents/trainer.py\u001b[0m in \u001b[0;36mcompute_single_action\u001b[0;34m(self, observation, state, prev_action, prev_reward, info, input_dict, policy_id, full_fetch, explore, timestep, episode, unsquash_action, clip_action, unsquash_actions, clip_actions, **kwargs)\u001b[0m\n\u001b[1;32m   1130\u001b[0m         \u001b[0;31m# Individual args.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1132\u001b[0;31m             action, state, extra = policy.compute_single_action(\n\u001b[0m\u001b[1;32m   1133\u001b[0m                 \u001b[0mobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m                 \u001b[0mstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/ray/rllib/policy/policy.py\u001b[0m in \u001b[0;36mcompute_single_action\u001b[0;34m(self, obs, state, prev_action, prev_reward, info, input_dict, episode, explore, timestep, **kwargs)\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0mepisodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m         out = self.compute_actions_from_input_dict(\n\u001b[0m\u001b[1;32m    244\u001b[0m             \u001b[0minput_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSampleBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0mepisodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/ray/rllib/policy/torch_policy.py\u001b[0m in \u001b[0;36mcompute_actions_from_input_dict\u001b[0;34m(self, input_dict, explore, timestep, **kwargs)\u001b[0m\n\u001b[1;32m    300\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mstate_batches\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m             return self._compute_action_helper(input_dict, state_batches,\n\u001b[0m\u001b[1;32m    303\u001b[0m                                                seq_lens, explore, timestep)\n\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/ray/rllib/utils/threading.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *a, **k)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"has no attribute '_lock'\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/ray/rllib/policy/torch_policy.py\u001b[0m in \u001b[0;36m_compute_action_helper\u001b[0;34m(self, input_dict, state_batches, seq_lens, explore, timestep)\u001b[0m\n\u001b[1;32m    364\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m                 \u001b[0mdist_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdist_class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m                 dist_inputs, state_out = self.model(input_dict, state_batches,\n\u001b[0m\u001b[1;32m    367\u001b[0m                                                     seq_lens)\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/ray/rllib/models/modelv2.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, input_dict, state, seq_lens)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrestored\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_lens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m         if ((not isinstance(res, list) and not isinstance(res, tuple))\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/ray/rllib/models/torch/recurrent_net.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_dict, state, seq_lens)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lstm_use_prev_action\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mDiscrete\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMultiDiscrete\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m                 prev_a = one_hot(input_dict[SampleBatch.PREV_ACTIONS].float(),\n\u001b[0m\u001b[1;32m    194\u001b[0m                                  self.action_space)\n\u001b[1;32m    195\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/ray/rllib/utils/torch_ops.py\u001b[0m in \u001b[0;36mone_hot\u001b[0;34m(x, space)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMultiDiscrete\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         return torch.cat(\n\u001b[0;32m--> 188\u001b[0;31m             [\n\u001b[0m\u001b[1;32m    189\u001b[0m                 \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/ray/rllib/utils/torch_ops.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    187\u001b[0m         return torch.cat(\n\u001b[1;32m    188\u001b[0m             [\n\u001b[0;32m--> 189\u001b[0;31m                 \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             ],\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for dimension 1 with size 1"
     ]
    }
   ],
   "source": [
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "\n",
    "checkpoints = results.get_trial_checkpoints_paths(\n",
    "    trial=results.get_best_trial(\"episode_reward_mean\", mode=\"max\"),\n",
    "    metric=\"episode_reward_mean\")\n",
    "\n",
    "checkpoint_path = checkpoints[0][0]\n",
    "trainer = PPOTrainer(config)\n",
    "trainer.restore(checkpoint_path)\n",
    "\n",
    "# Inference loop.\n",
    "env = StatelessCartPoleMD()\n",
    "obs = env.reset()\n",
    "# range(2) b/c h- and c-states of the LSTM.\n",
    "lstm_cell_size = 256\n",
    "init_state = state = [\n",
    "        np.zeros([lstm_cell_size], np.float32) for _ in range(2)\n",
    "]\n",
    "\n",
    "# Run manual inference loop for n episodes.\n",
    "for _ in range(10):\n",
    "    episode_reward = 0\n",
    "    reward = 0.0\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    state = init_state\n",
    "    prev_a = [0, 0]\n",
    "    prev_r = 0.0\n",
    "\n",
    "    while not done:\n",
    "        a, state_out, _ = trainer.compute_single_action(obs, state, prev_action=prev_a, prev_reward=prev_r)\n",
    "        obs, reward, done, _ = env.step(a)\n",
    "        episode_reward += reward\n",
    "        prev_a = a\n",
    "        prev_r = reward\n",
    "        state = state_out\n",
    "\n",
    "    print(f\"Episode reward={episode_reward}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3854e610",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
